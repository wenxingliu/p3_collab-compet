{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as the continuous control project, I used DDPG framework in this collaboration and competition project. Since both players have the same objective in the game, which is to keep the ball in play as much as possible. The environment given in this project takes both players observations at the same time, and outputs rewards. Since this game is \"symmetric\" in the sense that both players can learn from each other's experience, and share the same goal. We can just consider the two players are independent of each other, almost like two agents are playing sepparately in their own environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution adopts a DDPG framework, where we use an actor net to select actions, and use a critic net to evaluate state-action values. During training, the agent uses a traget actor net, and a target critic net to guide the training process, which also stablizes learning. The target networks have exactly same structure and initial weights as the local networks, and they are soft-updated using the weights of local actor net and critic net using a soft update value of 1e-3. At each step, the agent collect new experiences from two players, and added them to replay buffer. The agent then samples a batch of 512 past experiences, and use the batch for updating actor and critic nets. In the solution proposed in this repo, the agent repeats this update 5 times each time it collects new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list below shows parameters used in this solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparams import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The replay buffer has a size of 1000000\n",
      "At each step, the replay buffer samples a batch of 512 past experiences.\n",
      "The actor's learning rate is 0.0001\n",
      "The critic's learning rate is 0.0001\n",
      "The soft update rate is 0.001\n",
      "The agent updates/learns actor net and critic net 5 times at each step.\n"
     ]
    }
   ],
   "source": [
    "print(\"The replay buffer has a size of\", MEMORY_BUFFER)\n",
    "print(\"At each step, the replay buffer samples a batch of %d past experiences.\" % BATCH_SIZE)\n",
    "print(\"The actor's learning rate is\", LR_ACTOR)\n",
    "print(\"The critic's learning rate is\", LR_CRITIC)\n",
    "print(\"The soft update rate is\", TAU)\n",
    "print(\"The agent updates/learns actor net and critic net %d times at each step.\" % UPDATE_FREQUENCY_PER_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the solution proposed in this repo, the agent is able to solve the environment in less than 1500 episodes. And the average score in a 100-episode-average-score stays above 0.50, and hit an average score of 1.50 at about 2100 episodes. The training scores are shown in the plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scores](DDPG_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate the solution. We ran the agent for 100 episodes, and obtained an average score of 1.55. From the plot we can tell the agent is not very stable, in some episodes, it was not able to score at all; while in some episodes, it was able to get scores as high as 2.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scores](Test_Agent_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore possiblities of stablizing agent's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
